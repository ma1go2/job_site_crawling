{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056faf9-f9d4-4122-84b0-1caa93cf9625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import time\n",
    "\n",
    "# --- Selenium ê´€ë ¨ (ì¡ì½”ë¦¬ì•„ìš©) ---\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ==========================================\n",
    "# Selenium ë“œë¼ì´ë²„ ì„¤ì • (ì¡ì½”ë¦¬ì•„ ì „ìš©)\n",
    "# ==========================================\n",
    "@st.cache_resource\n",
    "def get_driver():\n",
    "    options = Options()\n",
    "    # ë¶ˆí•„ìš”í•œ ì—ëŸ¬ ë¡œê·¸ ì œê±°\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-logging\"]) \n",
    "    # ë¸Œë¼ìš°ì € ì°½ ìˆ¨ê¸°ê¸° (ì†ë„ í–¥ìƒ)\n",
    "    options.add_argument(\"--headless\") \n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    return driver\n",
    "\n",
    "# ==========================================\n",
    "# ë°ì´í„° ë¡œë“œ (CSV íŒŒì¼ ì—°ë™)\n",
    "# ==========================================\n",
    "@st.cache_data\n",
    "def load_data():\n",
    "    # ì¸í¬ë£¨íŠ¸\n",
    "    try:\n",
    "        incruit_rgn = pd.read_csv(\"ì¸í¬ë£¨íŠ¸_ì§€ì—­ì½”ë“œ.csv\")\n",
    "        incruit_occ = pd.read_csv(\"ì¸í¬ë£¨íŠ¸_ì§ì¢…_ì§ë¬´ì½”ë“œ.csv\")\n",
    "    except FileNotFoundError:\n",
    "        incruit_rgn = pd.DataFrame()\n",
    "        incruit_occ = pd.DataFrame()\n",
    "    \n",
    "    # ì‚¬ëŒì¸\n",
    "    try:\n",
    "        saramin_rgn = pd.read_csv(\"ì‚¬ëŒì¸_ì§€ì—­ì½”ë“œ.csv\")\n",
    "        saramin_occ = pd.read_csv(\"ì‚¬ëŒì¸_ì§ë¬´ì½”ë“œ.csv\")\n",
    "    except FileNotFoundError:\n",
    "        saramin_rgn = pd.DataFrame()\n",
    "        saramin_occ = pd.DataFrame()\n",
    "\n",
    "    # ì¡ì½”ë¦¬ì•„\n",
    "    try:\n",
    "        jobkorea_rgn = pd.read_csv(\"ì¡ì½”ë¦¬ì•„_ì§€ì—­ì½”ë“œ.csv\")\n",
    "        jobkorea_occ = pd.read_csv(\"ì¡ì½”ë¦¬ì•„_ì§ë¬´ì½”ë“œ.csv\")\n",
    "    except FileNotFoundError:\n",
    "        jobkorea_rgn = pd.DataFrame()\n",
    "        jobkorea_occ = pd.DataFrame()\n",
    "        \n",
    "    return incruit_rgn, incruit_occ, saramin_rgn, saramin_occ, jobkorea_rgn, jobkorea_occ\n",
    "\n",
    "# ==========================================\n",
    "# ì¸í¬ë£¨íŠ¸ í¬ë¡¤ë§ (Requests ë°©ì‹)\n",
    "# ==========================================\n",
    "def set_incruit_url(page=1, rgn1='', rgn2='', rgn3='', occ1='', occ2='', occ3='', keyword=''):\n",
    "    occ1_list = [f'&occ1={i}' for i in str(occ1).split(',') if i and i != 'nan']\n",
    "    occ2_list = [f'&occ2={i}' for i in (occ2 if isinstance(occ2, list) else str(occ2).split(',')) if i]\n",
    "    occ3_list = [f'&occ3={i}' for i in (occ3 if isinstance(occ3, list) else str(occ3).split(',')) if i]\n",
    "    rgn2_list = [f'&rgn2={i}' for i in (rgn2 if isinstance(rgn2, list) else str(rgn2).split(',')) if i]\n",
    "    rgn3_list = [f'&rgn3={i}' for i in (rgn3 if isinstance(rgn3, list) else str(rgn3).split(',')) if i]\n",
    "    kw_str = f\"&kw={keyword}\" if keyword else \"\"\n",
    "\n",
    "    base_url = (\n",
    "        f\"https://job.incruit.com/jobdb_list/searchjob.asp?\"\n",
    "        + \"\".join(occ1_list) + \"\".join(occ2_list) + \"\".join(occ3_list)\n",
    "        + f\"{f'&rgn1={rgn1}' if rgn1 else ''}\"\n",
    "        + \"\".join(rgn2_list) + \"\".join(rgn3_list)\n",
    "        + kw_str + f\"&page={page}\"\n",
    "    )\n",
    "    return base_url.replace('?&', '?')\n",
    "\n",
    "def crawl_incruit_site(pages, rgn_codes, occ_codes, keyword):\n",
    "    results = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    rgn1 = rgn_codes.get('rgn1', '')\n",
    "    rgn2 = rgn_codes.get('rgn2', [])\n",
    "    rgn3 = rgn_codes.get('rgn3', [])\n",
    "    occ1 = occ_codes.get('occ1', '')\n",
    "    occ2 = occ_codes.get('occ2', [])\n",
    "    occ3 = occ_codes.get('occ3', [])\n",
    "\n",
    "    progress_bar = st.progress(0)\n",
    "    status_text = st.empty()\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        status_text.text(f\"ì¸í¬ë£¨íŠ¸ {page}/{pages} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\")\n",
    "        url = set_incruit_url(page, rgn1, rgn2, rgn3, occ1, occ2, occ3, keyword=keyword)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            items = soup.select('.c_row')\n",
    "            \n",
    "            if not items: break\n",
    "\n",
    "            for item in items:\n",
    "                try:\n",
    "                    top_area = item.select('.cl_top')\n",
    "                    if len(top_area) < 2: continue\n",
    "                    company = top_area[0].select_one('.cpname').text.strip()\n",
    "                    title = top_area[1].find('a').text.strip()\n",
    "                    link = top_area[1].find('a')['href']\n",
    "                    deadline = item.select_one('.cell_last .cl_btm span').text.strip()\n",
    "                    if '~' in deadline:\n",
    "                        deadline = deadline.lstrip('~').replace(' ','')\n",
    "                    location = item.select_one('.cl_md').select('span')[0].text.strip()\n",
    "                    experience = item.select_one('.cl_md').select('span')[1].text.strip()\n",
    "                    education = item.select_one('.cl_md').select('span')[2].text.strip()\n",
    "                    \n",
    "                    results.append({\n",
    "                        'ì‚¬ì´íŠ¸': 'ì¸í¬ë£¨íŠ¸',\n",
    "                        'ê³µê³ ëª…': title, \n",
    "                        'íšŒì‚¬ëª…': company, \n",
    "                        'ë§í¬': link,\n",
    "                        'ì§€ì—­': location,\n",
    "                        'ë§ˆê°ì¼': deadline,\n",
    "                        'ê²½ë ¥': experience,\n",
    "                        'í•™ë ¥': education\n",
    "                    })\n",
    "                except: continue\n",
    "            progress_bar.progress(page / pages)\n",
    "            time.sleep(0.3)\n",
    "        except Exception as e:\n",
    "            st.error(f\"ì¸í¬ë£¨íŠ¸ ì—ëŸ¬: {e}\")\n",
    "            break\n",
    "            \n",
    "    status_text.empty()\n",
    "    progress_bar.empty()\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ==========================================\n",
    "# ì‚¬ëŒì¸ í¬ë¡¤ë§ (Requests ë°©ì‹)\n",
    "# ==========================================\n",
    "def set_saramin_url(keyword, page=1, loc_cd='', cat_mcls='', cat_kewd=''):\n",
    "    base_url = (\n",
    "        f\"https://www.saramin.co.kr/zf_user/search?searchType=search\"\n",
    "        f\"{f'&searchword={keyword}' if keyword else ''}\"\n",
    "        f\"{f'&loc_mcd={loc_cd}' if loc_cd else ''}\"\n",
    "        f\"{f'&cat_mcls={cat_mcls}' if cat_mcls else ''}\"\n",
    "        f\"{f'&cat_kewd={cat_kewd}' if cat_kewd else ''}\"\n",
    "        f\"&recruitPage={page}\"\n",
    "    )\n",
    "    return base_url\n",
    "\n",
    "def crawl_saramin_site(pages, loc_codes, cat_codes, keyword):\n",
    "    results = []\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    loc_cd = \",\".join(map(str, loc_codes))\n",
    "    cat_mcls = \",\".join(map(str, cat_codes['mcls']))\n",
    "    cat_kewd = \",\".join(map(str, cat_codes['kewd']))\n",
    "\n",
    "    progress_bar = st.progress(0)\n",
    "    status_text = st.empty()\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        status_text.text(f\"ì‚¬ëŒì¸ {page}/{pages} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘...\")\n",
    "        url = set_saramin_url(keyword, page, loc_cd, cat_mcls, cat_kewd)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            items = soup.select('.item_recruit')\n",
    "            \n",
    "            if not items: break\n",
    "\n",
    "            for item in items:\n",
    "                try:\n",
    "                    title_tag = item.select_one('.job_tit a')\n",
    "                    title = title_tag.text.strip()\n",
    "                    link = \"https://www.saramin.co.kr\" + title_tag['href']\n",
    "                    company = item.select_one('.corp_name a').text.strip()\n",
    "                    deadline = item.select_one('.job_date .date').text.strip()\n",
    "                    if '~' in deadline:\n",
    "                        deadline = deadline.lstrip('~').replace(' ','').replace('/','.')\n",
    "                    conditions = item.select('.job_condition span')\n",
    "                    location = conditions[0].text if len(conditions) > 0 else \"\"\n",
    "                    experience = conditions[1].text if len(conditions) > 1 else \"\"\n",
    "                    education = conditions[2].text if len(conditions) > 2 else \"\"\n",
    "                    \n",
    "                    results.append({\n",
    "                        'ì‚¬ì´íŠ¸': 'ì‚¬ëŒì¸',\n",
    "                        'ê³µê³ ëª…': title,\n",
    "                        'íšŒì‚¬ëª…': company, \n",
    "                        'ë§í¬': link,\n",
    "                        'ì§€ì—­': location,\n",
    "                        'ë§ˆê°ì¼': deadline,\n",
    "                        'ê²½ë ¥': experience,\n",
    "                        'í•™ë ¥': education\n",
    "                    })\n",
    "                except: continue\n",
    "            progress_bar.progress(page / pages)\n",
    "            time.sleep(0.3)\n",
    "        except Exception as e:\n",
    "            st.error(f\"ì‚¬ëŒì¸ ì—ëŸ¬: {e}\")\n",
    "            break\n",
    "            \n",
    "    status_text.empty()\n",
    "    progress_bar.empty()\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ==========================================\n",
    "# ì¡ì½”ë¦¬ì•„ í¬ë¡¤ë§ (Selenium ë°©ì‹ - ì‹ ê·œ ë¡œì§ ì ìš©)\n",
    "# ==========================================\n",
    "def set_jobkorea_url(keyword, page=1, loc_cd='', duty_cd='', dkwrd_cd=''):\n",
    "    \"\"\"\n",
    "    ì¡ì½”ë¦¬ì•„ URL ìƒì„± (ì‹ ê·œ ë¡œì§ ì ìš©)\n",
    "    \"\"\"\n",
    "    base_url = (\n",
    "        f\"https://www.jobkorea.co.kr/Search/?stext={keyword}\"\n",
    "        f\"{f'&local={loc_cd}' if loc_cd else ''}\"\n",
    "        f\"{f'&duty={duty_cd}' if duty_cd else ''}\"\n",
    "        f\"{f'&dkwrd={dkwrd_cd}' if dkwrd_cd else ''}\"\n",
    "        f\"&Page_No={page}\"\n",
    "    )\n",
    "    return base_url\n",
    "\n",
    "def crawl_jobkorea_site(driver, pages, local_codes, duty_codes, dkwrd_codes, keyword):\n",
    "    results = []\n",
    "    \n",
    "    # ë¦¬ìŠ¤íŠ¸ë¥¼ ì½¤ë§ˆ êµ¬ë¶„ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
    "    loc_cd = \",\".join(map(str, local_codes))\n",
    "    duty_cd = \",\".join(map(str, duty_codes))\n",
    "    dkwrd_cd = \",\".join(map(str, dkwrd_codes))\n",
    "    \n",
    "    progress_bar = st.progress(0)\n",
    "    status_text = st.empty()\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        status_text.text(f\"ì¡ì½”ë¦¬ì•„ {page}/{pages} í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘ (ë¸Œë¼ìš°ì €)...\")\n",
    "        url = set_jobkorea_url(keyword, page, loc_cd, duty_cd, dkwrd_cd)\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)  # Selenium ì‚¬ìš©\n",
    "            time.sleep(3)    # ê³µê³  ë¦¬ìŠ¤íŠ¸ ë¡œë”© ëŒ€ê¸° (í•„ìš”ì‹œ ì¡°ì ˆ)\n",
    "            \n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            items = soup.select('div[data-sentry-component=\"CardJob\"]')\n",
    "            \n",
    "            if not items: break \n",
    "\n",
    "            for item in items:\n",
    "                try:\n",
    "                    # ê³µê³ ëª…\n",
    "                    title = item.select_one(\n",
    "                        '.Typography_variant_size18__344nw25.Typography_weight_medium__344nw2d'\n",
    "                    ).text.strip()\n",
    "                    \n",
    "                    # ë§í¬\n",
    "                    link = item.select_one(\n",
    "                        'a[href*=\"/Recruit/GI_Read/\"]'\n",
    "                    )['href']\n",
    "                    if not link.startswith('http'):\n",
    "                        link = 'https://www.jobkorea.co.kr' + link\n",
    "                    \n",
    "                    # íšŒì‚¬ëª…\n",
    "                    company = item.select_one('.Typography_variant_size16__344nw26.Typography_color_gray700__344nw2o').text.strip()\n",
    "                    \n",
    "                    # ë§ˆê°ì¼\n",
    "                    deadline = None\n",
    "                    for span in item.select('.Typography_variant_size13__344nw28.Typography_weight_regular__344nw2e.Typography_color_gray700__344nw2o'):\n",
    "                        text = span.text.strip()\n",
    "                        if 'ë§ˆê°' in text:\n",
    "                            deadline = text\n",
    "                            break\n",
    "                    if 'ë§ˆê°' in deadline:\n",
    "                        deadline = deadline.rstrip('ë§ˆê°').replace(' ','').replace('/','.')\n",
    "                        \n",
    "                    if not deadline: deadline = \"ìƒì‹œ\"\n",
    "\n",
    "                    # ìœ„ì¹˜\n",
    "                    location = item.select_one('.Typography_variant_size14__344nw27.Typography_color_gray900__344nw2m').text.strip()\n",
    "\n",
    "                    # ê²½ë ¥\n",
    "                    experience = item.select_one('.Typography_variant_size13__344nw28.Typography_color_gray700__344nw2o').text.strip()\n",
    "                    \n",
    "                    results.append({\n",
    "                        'ì‚¬ì´íŠ¸': 'ì¡ì½”ë¦¬ì•„',\n",
    "                        'ê³µê³ ëª…': title,\n",
    "                        'íšŒì‚¬ëª…': company,\n",
    "                        'ë§í¬': link,\n",
    "                        'ì§€ì—­': location,\n",
    "                        'ë§ˆê°ì¼': deadline,\n",
    "                        'ê²½ë ¥': experience,\n",
    "                        'í•™ë ¥': \"ì§ì ‘ í™•ì¸ í•„ìš”\"\n",
    "                    })\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            progress_bar.progress(page / pages)\n",
    "            \n",
    "        except Exception as e:\n",
    "            st.error(f\"ì¡ì½”ë¦¬ì•„ ì—ëŸ¬: {e}\")\n",
    "            break\n",
    "            \n",
    "    status_text.empty()\n",
    "    progress_bar.empty()\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# ==========================================\n",
    "# ì—‘ì…€ ë³€í™˜\n",
    "# ==========================================\n",
    "def convert_to_excel(df):\n",
    "    output = io.BytesIO()\n",
    "    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "        df.to_excel(writer, index=False, sheet_name='Result')\n",
    "    return output.getvalue()\n",
    "\n",
    "# ==========================================\n",
    "# ë©”ì¸ UI\n",
    "# ==========================================\n",
    "def main():\n",
    "    st.set_page_config(layout=\"wide\", page_title=\"í†µí•© ì±„ìš© í¬ë¡¤ëŸ¬ (Hybrid)\")\n",
    "    st.title(\"ğŸ•µï¸â€â™€ï¸ í†µí•© ì±„ìš© ê³µê³  í¬ë¡¤ëŸ¬ (Hybrid Ver.)\")\n",
    "    st.caption(\"ì¸í¬ë£¨íŠ¸/ì‚¬ëŒì¸: Requests | ì¡ì½”ë¦¬ì•„: Selenium\")\n",
    "\n",
    "    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ë°ì´í„° ì €ì¥ì†Œ ë§Œë“¤ê¸°)\n",
    "    if 'crawled_data' not in st.session_state:\n",
    "        st.session_state['crawled_data'] = pd.DataFrame()\n",
    "    \n",
    "    inc_rgn, inc_occ, sar_rgn, sar_occ, jk_rgn, jk_occ = load_data()\n",
    "    \n",
    "    # ì‚¬ì´ë“œë°” ì„¤ì •\n",
    "    st.sidebar.header(\"ê¸°ë³¸ ì„¤ì •\")\n",
    "    target_sites = st.sidebar.multiselect(\n",
    "        \"í¬ë¡¤ë§í•  ì‚¬ì´íŠ¸\", \n",
    "        [\"ì¸í¬ë£¨íŠ¸\", \"ì‚¬ëŒì¸\", \"ì¡ì½”ë¦¬ì•„\"], \n",
    "        default=[\"ì¸í¬ë£¨íŠ¸\", \"ì‚¬ëŒì¸\", \"ì¡ì½”ë¦¬ì•„\"]\n",
    "    )\n",
    "    common_keywords = st.sidebar.text_input(\"ê³µí†µ ê²€ìƒ‰ í‚¤ì›Œë“œ (ì¸í¬ë£¨íŠ¸ ì œì™¸)\")\n",
    "\n",
    "    # ì˜µì…˜ ì €ì¥ì†Œ\n",
    "    inc_opt = {'rgn': {'rgn1':'', 'rgn2':[], 'rgn3':[]}, 'occ': {'occ1':'', 'occ2':[], 'occ3':[]}}\n",
    "    sar_opt = {'loc': [], 'cat': {'mcls': [], 'kewd': []}}\n",
    "    jk_opt = {'local': [], 'duty': [], 'dkwrd': []} # ì¡ì½”ë¦¬ì•„ êµ¬ì¡° ë³€ê²½: duty(ìƒìœ„), dkwrd(í•˜ìœ„)\n",
    "    \n",
    "    inc_p, sar_p, jk_p = 1, 1, 1\n",
    "\n",
    "    # [ì¸í¬ë£¨íŠ¸ UI]\n",
    "    if \"ì¸í¬ë£¨íŠ¸\" in target_sites:\n",
    "        with st.sidebar.expander(\"ğŸŸ¦ ì¸í¬ë£¨íŠ¸ ì„¤ì •\", expanded=False):\n",
    "            if inc_rgn.empty: st.error(\"ì¸í¬ë£¨íŠ¸ CSV íŒŒì¼ ëˆ„ë½\")\n",
    "            else:\n",
    "                inc_p = st.number_input(\"í˜ì´ì§€ ìˆ˜ (1page = 60ê±´)\", 1, 50, 1, key='ip')\n",
    "                # ì§ë¬´\n",
    "                i_o1 = st.selectbox(\"ì§ì¢…(ìƒìœ„)\", [\"ì „ì²´\"]+list(inc_occ['occ1_nm'].unique()), key='io1')\n",
    "                if i_o1 != \"ì „ì²´\":\n",
    "                    inc_opt['occ']['occ1'] = inc_occ[inc_occ['occ1_nm']==i_o1]['occ1'].iloc[0]\n",
    "                    i_o2 = st.selectbox(\"ì§ì¢…(ì¤‘ìœ„)\", [\"ì „ì²´\"]+list(inc_occ[inc_occ['occ1_nm']==i_o1]['occ2_nm'].unique()), key='io2')\n",
    "                    if i_o2 != \"ì „ì²´\":\n",
    "                        i_o3_df = inc_occ[(inc_occ['occ1_nm']==i_o1) & (inc_occ['occ2_nm']==i_o2)]\n",
    "                        i_o3_sel = st.multiselect(\"ì§ë¬´(ìƒì„¸)\", i_o3_df['occ3_nm'].unique(), key='io3')\n",
    "                        inc_opt['occ']['occ3'] = i_o3_df[i_o3_df['occ3_nm'].isin(i_o3_sel)]['occ3'].tolist()\n",
    "                # ì§€ì—­\n",
    "                i_r1 = st.selectbox(\"ì‹œ/ë„\", [\"ì „ì²´\"]+list(inc_rgn['rgn2_nm'].unique()), key='ir1')\n",
    "                if i_r1 == \"ì „ì²´\": inc_opt['rgn']['rgn1'] = 149\n",
    "                else:\n",
    "                    inc_opt['rgn']['rgn2'] = [inc_rgn[inc_rgn['rgn2_nm']==i_r1]['rgn2'].iloc[0]]\n",
    "                    i_r2_sel = st.multiselect(\"êµ¬/êµ°\", inc_rgn[inc_rgn['rgn2_nm']==i_r1]['rgn3_nm'].unique(), key='ir2')\n",
    "                    if i_r2_sel:\n",
    "                        inc_opt['rgn']['rgn3'] = inc_rgn[(inc_rgn['rgn2_nm']==i_r1) & (inc_rgn['rgn3_nm'].isin(i_r2_sel))]['rgn3'].tolist()\n",
    "\n",
    "    # [ì‚¬ëŒì¸ UI]\n",
    "    if \"ì‚¬ëŒì¸\" in target_sites:\n",
    "        with st.sidebar.expander(\"ğŸŸ§ ì‚¬ëŒì¸ ì„¤ì •\", expanded=False):\n",
    "            if sar_rgn.empty: st.error(\"ì‚¬ëŒì¸ CSV íŒŒì¼ ëˆ„ë½\")\n",
    "            else:\n",
    "                sar_p = st.number_input(\"í˜ì´ì§€ ìˆ˜ (1page = 40ê±´)\", 1, 50, 1, key='sp')\n",
    "                # ì§ë¬´\n",
    "                s_o1 = st.selectbox(\"ì§ë¬´(ìƒìœ„)\", [\"ì „ì²´\"]+list(sar_occ['cat_mcls_nm'].unique()), key='so1')\n",
    "                if s_o1 != \"ì „ì²´\":\n",
    "                    sar_opt['cat']['mcls'] = [sar_occ[sar_occ['cat_mcls_nm']==s_o1]['cat_mcls'].iloc[0]]\n",
    "                    s_o2_sel = st.multiselect(\"ì§ë¬´(í‚¤ì›Œë“œ)\", sar_occ[sar_occ['cat_mcls_nm']==s_o1]['cat_kewd_nm'].unique(), key='so2')\n",
    "                    sar_opt['cat']['kewd'] = sar_occ[sar_occ['cat_kewd_nm'].isin(s_o2_sel)]['cat_kewd'].tolist()\n",
    "                # ì§€ì—­\n",
    "                s_r1 = st.selectbox(\"ì‹œ/ë„\", [\"ì „ì²´\"]+list(sar_rgn['loc_cd1_nm'].unique()), key='sr1')\n",
    "                if s_r1 != \"ì „ì²´\":\n",
    "                    s_r2_sel = st.multiselect(\"êµ¬/êµ°\", sar_rgn[sar_rgn['loc_cd1_nm']==s_r1]['loc_cd2_nm'].unique(), key='sr2')\n",
    "                    sar_opt['loc'] = sar_rgn[sar_rgn['loc_cd2_nm'].isin(s_r2_sel)]['loc_cd2'].tolist() if s_r2_sel else [sar_rgn[sar_rgn['loc_cd1_nm']==s_r1]['loc_cd1'].iloc[0]]\n",
    "\n",
    "    # [ì¡ì½”ë¦¬ì•„ UI]\n",
    "    if \"ì¡ì½”ë¦¬ì•„\" in target_sites:\n",
    "        with st.sidebar.expander(\"ğŸŸª ì¡ì½”ë¦¬ì•„ ì„¤ì • (Selenium)\", expanded=False):\n",
    "            if jk_rgn.empty: st.error(\"ì¡ì½”ë¦¬ì•„ CSV íŒŒì¼ ëˆ„ë½\")\n",
    "            else:\n",
    "                jk_p = st.number_input(\"í˜ì´ì§€ ìˆ˜ (1page = 20ê±´)\", 1, 50, 1, key='jp')\n",
    "                \n",
    "                st.markdown(\"**ì§ë¬´**\")\n",
    "                \n",
    "                # 1. ìƒìœ„ ì§ë¬´ ì„ íƒ (UIìš©)\n",
    "                j_upper = st.selectbox(\"ì§ë¬´(ìƒìœ„)\", [\"ì „ì²´\"] + list(jk_occ['cd_nm'].unique()), key='jo1')\n",
    "                \n",
    "                if j_upper != \"ì „ì²´\":\n",
    "                    # 2. ìƒìœ„ ì§ë¬´(cd_nm)ì— ì†í•œ 'ëª¨ë“ ' ì¤‘ìœ„ ì§ë¬´ ì½”ë“œ(duty_cd)ë¥¼ ê°€ì ¸ì˜´\n",
    "                    selected_duty_cds = jk_occ[jk_occ['cd_nm'] == j_upper]['duty_cd'].unique().tolist()\n",
    "                    jk_opt['duty'] = selected_duty_cds\n",
    "                    \n",
    "                    # 3. í•˜ìœ„ ì§ë¬´(í‚¤ì›Œë“œ) ì„ íƒ\n",
    "                    j_lower_options = jk_occ[jk_occ['cd_nm'] == j_upper]['dkwrd_cd_nm'].unique()\n",
    "                    j_lower_sel = st.multiselect(\"ì§ë¬´(ìƒì„¸)\", j_lower_options, key='jo2')\n",
    "                    \n",
    "                    if j_lower_sel:\n",
    "                        # 4. ì„ íƒëœ í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” ì½”ë“œ(dkwrd_cd) ë§¤í•‘\n",
    "                        selected_dkwrd_cds = jk_occ[\n",
    "                            (jk_occ['cd_nm'] == j_upper) & \n",
    "                            (jk_occ['dkwrd_cd_nm'].isin(j_lower_sel))\n",
    "                        ]['dkwrd_cd'].astype(int).tolist()\n",
    "                        jk_opt['dkwrd'] = selected_dkwrd_cds\n",
    "\n",
    "                # ì§€ì—­ ì„¤ì • (ê¸°ì¡´ ìœ ì§€)\n",
    "                st.markdown(\"**ì§€ì—­**\")\n",
    "                j_r1 = st.selectbox(\"ì‹œ/ë„\", [\"ì „ì²´\"] + list(jk_rgn['loc_cd1_nm'].unique()), key='jr1')\n",
    "                if j_r1 != \"ì „ì²´\":\n",
    "                    j_r2_sel = st.multiselect(\"êµ¬/êµ°\", jk_rgn[jk_rgn['loc_cd1_nm'] == j_r1]['loc_cd2_nm'].unique(), key='jr2')\n",
    "                    if j_r2_sel:\n",
    "                        jk_opt['local'] = jk_rgn[jk_rgn['loc_cd2_nm'].isin(j_r2_sel)]['loc_cd2'].tolist()\n",
    "                    else:\n",
    "                        jk_opt['local'] = [jk_rgn[jk_rgn['loc_cd1_nm'] == j_r1]['loc_cd1'].iloc[0]]\n",
    "                        \n",
    "    # ì‹¤í–‰\n",
    "    if st.button(\"ğŸš€ í¬ë¡¤ë§ ì‹œì‘\", type=\"primary\"):\n",
    "        # ìƒˆë¡œ ìˆ˜ì§‘ ì‹œì‘ ì‹œ ë¹ˆ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì‹œì‘\n",
    "        new_results = pd.DataFrame()\n",
    "\n",
    "        for common_keyword in common_keywords.split(','):\n",
    "            # 1. ì¸í¬ë£¨íŠ¸\n",
    "            if \"ì¸í¬ë£¨íŠ¸\" in target_sites:\n",
    "                df = crawl_incruit_site(inc_p, inc_opt['rgn'], inc_opt['occ'], common_keyword)\n",
    "                if not df.empty: new_results = pd.concat([new_results, df])\n",
    "                \n",
    "            # 2. ì‚¬ëŒì¸\n",
    "            if \"ì‚¬ëŒì¸\" in target_sites:\n",
    "                df = crawl_saramin_site(sar_p, sar_opt['loc'], sar_opt['cat'], common_keyword)\n",
    "                if not df.empty: new_results = pd.concat([new_results, df])\n",
    "                \n",
    "            # 3. ì¡ì½”ë¦¬ì•„\n",
    "            if \"ì¡ì½”ë¦¬ì•„\" in target_sites:\n",
    "                with st.spinner(\"ì¡ì½”ë¦¬ì•„ ìˆ˜ì§‘ì„ ìœ„í•´ ë¸Œë¼ìš°ì €ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...\"):\n",
    "                    driver = get_driver()\n",
    "                \n",
    "                df = crawl_jobkorea_site(\n",
    "                    driver, \n",
    "                    jk_p, \n",
    "                    jk_opt['local'], \n",
    "                    jk_opt['duty'], \n",
    "                    jk_opt['dkwrd'], \n",
    "                    common_keyword\n",
    "                )\n",
    "                if not df.empty: new_results = pd.concat([new_results, df])\n",
    "\n",
    "        new_results = new_results.drop_duplicates(subset=['ì‚¬ì´íŠ¸','ê³µê³ ëª…','íšŒì‚¬ëª…','ë§í¬'])\n",
    "        # ìˆ˜ì§‘ëœ ê²°ê³¼ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥\n",
    "        st.session_state['crawled_data'] = new_results\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # í™”ë©´ ì¶œë ¥ (ë²„íŠ¼ í´ë¦­ ì—¬ë¶€ì™€ ìƒê´€ì—†ì´ ì„¸ì…˜ì— ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¶œë ¥)\n",
    "    # ---------------------------------------------------------\n",
    "    if not st.session_state['crawled_data'].empty:\n",
    "        st.success(f"ì´ {len(st.session_state['crawled_data'])}ê±´ì˜ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.")\n",
    "        st.dataframe(\n",
    "            st.session_state['crawled_data'],\n",
    "            column_config={\n",
    "                "ë§í¬": st.column_config.LinkColumn(\n",
    "                    label="ê³µê³  ë§í¬",\n",            
    "                    help="í´ë¦­ ì‹œ í•´ë‹¹ ê³µê³ ë¡œ ì´ë™í•©ë‹ˆë‹¤.", # ë§ˆìš°ìŠ¤ ì˜¬ë ¸ì„ ë•Œ ì„¤ëª…\n",
    "                    display_text="ë°”ë¡œê°€ê¸°"         # [ì„ íƒ] URL ëŒ€ì‹  ë³´ì—¬ì¤„ í…ìŠ¤íŠ¸ (ì´ ì¤„ì„ ì§€ìš°ë©´ URLì´ ê·¸ëŒ€ë¡œ ë³´ì„)\n",
    "                )\n",
    "            },\n",
    "            hide_index=True,  # ì¸ë±ìŠ¤ ë²ˆí˜¸ ìˆ¨ê¸°ê¸° (ì„ íƒì‚¬í•­)\n",
    "            use_container_width=True # í‘œê°€ ê°€ë¡œë¡œ ê½‰ì°¸\n",
    "        )\n",
    "    elif st.button(\"ê²°ê³¼ ì´ˆê¸°í™”\"): # ì„ íƒ ì‚¬í•­: ê²°ê³¼ë¥¼ ì§€ìš°ê³  ì‹¶ì„ ë•Œ\n",
    "        st.session_state['crawled_data'] = pd.DataFrame()\n",
    "        st.rerun()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
